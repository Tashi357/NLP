{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2d699c-724c-4624-9c54-82098fe7227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5df9669-50b6-467d-a857-b5b6c69e8219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: I love this product! It's amazing.\n",
      "Text After HTML Removal: i love this product its amazing\n",
      "Tokens: ['i', 'love', 'this', 'product', 'its', 'amazing']\n",
      "Filtered Tokens: ['love', 'product', 'amazing']\n",
      "Stemmed Tokens: ['love', 'product', 'amaz']\n",
      "Lemmatized Tokens: ['love', 'product', 'amaze']\n",
      "--------------------------------------------------\n",
      "Original Document: <html><head><title>Title</title></head><body><p>This is a <b>sample</b> paragraph.</p></body></html>\n",
      "Text After HTML Removal: titlethis is a sample paragraph\n",
      "Tokens: ['titlethis', 'is', 'a', 'sample', 'paragraph']\n",
      "Filtered Tokens: ['titlethis', 'sample', 'paragraph']\n",
      "Stemmed Tokens: ['titlethi', 'sampl', 'paragraph']\n",
      "Lemmatized Tokens: ['titlethis', 'sample', 'paragraph']\n",
      "--------------------------------------------------\n",
      "Original Document: He bought 123 apples and 456 oranges.\n",
      "Text After HTML Removal: he bought  apples and  oranges\n",
      "Tokens: ['he', 'bought', 'apples', 'and', 'oranges']\n",
      "Filtered Tokens: ['bought', 'apples', 'oranges']\n",
      "Stemmed Tokens: ['bought', 'appl', 'orang']\n",
      "Lemmatized Tokens: ['bought', 'apple', 'orange']\n",
      "--------------------------------------------------\n",
      "Original Document: Running is a great way to stay fit.\n",
      "Text After HTML Removal: running is a great way to stay fit\n",
      "Tokens: ['running', 'is', 'a', 'great', 'way', 'to', 'stay', 'fit']\n",
      "Filtered Tokens: ['running', 'great', 'way', 'stay', 'fit']\n",
      "Stemmed Tokens: ['run', 'great', 'way', 'stay', 'fit']\n",
      "Lemmatized Tokens: ['run', 'great', 'way', 'stay', 'fit']\n",
      "--------------------------------------------------\n",
      "Original Document: The quick brown fox jumps over the lazy dog.\n",
      "Text After HTML Removal: the quick brown fox jumps over the lazy dog\n",
      "Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Filtered Tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "Stemmed Tokens: ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
      "Lemmatized Tokens: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
      "--------------------------------------------------\n",
      "Original Document: She sells sea shells by the sea shore.\n",
      "Text After HTML Removal: she sells sea shells by the sea shore\n",
      "Tokens: ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']\n",
      "Filtered Tokens: ['sells', 'sea', 'shells', 'sea', 'shore']\n",
      "Stemmed Tokens: ['sell', 'sea', 'shell', 'sea', 'shore']\n",
      "Lemmatized Tokens: ['sell', 'sea', 'shell', 'sea', 'shore']\n",
      "--------------------------------------------------\n",
      "Original Document: COVID-19 has impacted global economies significantly.\n",
      "Text After HTML Removal: covid has impacted global economies significantly\n",
      "Tokens: ['covid', 'has', 'impacted', 'global', 'economies', 'significantly']\n",
      "Filtered Tokens: ['covid', 'impacted', 'global', 'economies', 'significantly']\n",
      "Stemmed Tokens: ['covid', 'impact', 'global', 'economi', 'significantli']\n",
      "Lemmatized Tokens: ['covid', 'impact', 'global', 'economy', 'significantly']\n",
      "--------------------------------------------------\n",
      "Original Document: The new iPhone 13 features a sleek design and powerful performance.\n",
      "Text After HTML Removal: the new iphone  features a sleek design and powerful performance\n",
      "Tokens: ['the', 'new', 'iphone', 'features', 'a', 'sleek', 'design', 'and', 'powerful', 'performance']\n",
      "Filtered Tokens: ['new', 'iphone', 'features', 'sleek', 'design', 'powerful', 'performance']\n",
      "Stemmed Tokens: ['new', 'iphon', 'featur', 'sleek', 'design', 'power', 'perform']\n",
      "Lemmatized Tokens: ['new', 'iphone', 'feature', 'sleek', 'design', 'powerful', 'performance']\n",
      "--------------------------------------------------\n",
      "Original Document: Artificial intelligence and machine learning are transforming industries.\n",
      "Text After HTML Removal: artificial intelligence and machine learning are transforming industries\n",
      "Tokens: ['artificial', 'intelligence', 'and', 'machine', 'learning', 'are', 'transforming', 'industries']\n",
      "Filtered Tokens: ['artificial', 'intelligence', 'machine', 'learning', 'transforming', 'industries']\n",
      "Stemmed Tokens: ['artifici', 'intellig', 'machin', 'learn', 'transform', 'industri']\n",
      "Lemmatized Tokens: ['artificial', 'intelligence', 'machine', 'learn', 'transform', 'industry']\n",
      "--------------------------------------------------\n",
      "Original Document: HTML, CSS, and JavaScript are essential technologies for web development.\n",
      "Text After HTML Removal: html css and javascript are essential technologies for web development\n",
      "Tokens: ['html', 'css', 'and', 'javascript', 'are', 'essential', 'technologies', 'for', 'web', 'development']\n",
      "Filtered Tokens: ['html', 'css', 'javascript', 'essential', 'technologies', 'web', 'development']\n",
      "Stemmed Tokens: ['html', 'css', 'javascript', 'essenti', 'technolog', 'web', 'develop']\n",
      "Lemmatized Tokens: ['html', 'cs', 'javascript', 'essential', 'technology', 'web', 'development']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Initialize the Porter Stemmer and WordNet Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample corpus\n",
    "sentencescombined_sentences = [\"I love this product! It's amazing.\",\n",
    "                               \"<html><head><title>Title</title></head><body><p>This is a <b>sample</b> paragraph.</p></body></html>\",\n",
    "                               \"He bought 123 apples and 456 oranges.\",\n",
    "                               \"Running is a great way to stay fit.\",\n",
    "                               \"The quick brown fox jumps over the lazy dog.\",\n",
    "                               \"She sells sea shells by the sea shore.\", \n",
    "                               \"COVID-19 has impacted global economies significantly.\",\n",
    "                               \"The new iPhone 13 features a sleek design and powerful performance.\",\n",
    "                               \"Artificial intelligence and machine learning are transforming industries.\",\n",
    "                               \"HTML, CSS, and JavaScript are essential technologies for web development.\" ]\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_tokens]\n",
    "    return {\n",
    "        'original': text,\n",
    "        'tokens': tokens,\n",
    "        'filtered_tokens': filtered_tokens,\n",
    "        'stemmed_tokens': stemmed_tokens,\n",
    "        'lemmatized_tokens': lemmatized_tokens\n",
    "    }\n",
    "\n",
    "# Function to get the part of speech tag for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Process each document in the corpus\n",
    "for document in sentencescombined_sentences:\n",
    "    result = preprocess_text(document)\n",
    "    print(\"Original Document:\", document)\n",
    "    print(\"Text After HTML Removal:\", result['original'])\n",
    "    print(\"Tokens:\", result['tokens'])\n",
    "    print(\"Filtered Tokens:\", result['filtered_tokens'])\n",
    "    print(\"Stemmed Tokens:\", result['stemmed_tokens'])\n",
    "    print(\"Lemmatized Tokens:\", result['lemmatized_tokens'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e7417-faa4-44fa-8c68-f52c92a44c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
